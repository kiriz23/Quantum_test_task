{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2830b71d-3646-49cc-81da-98b23d4cb09d",
   "metadata": {},
   "source": [
    "# Made by Kyrylo Krocha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e59e3e-1398-49b2-9410-a01bba97baba",
   "metadata": {},
   "source": [
    "### Some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a44564b6-1e02-4254-b285-cedbf061fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r \"D:\\\\InterviewProject\\\\Task1\\\\requirements.TXT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa09182a-670b-4cbb-a264-e062a241aa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "import pandas as pd\n",
    "from transformers import pipeline, BertForSequenceClassification, BertTokenizerFast\n",
    "from torch.utils.data import Dataset\n",
    "import accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c8d0bdb-11fa-4735-ae6c-18bd81b9d3ca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c619c4-546c-4f61-aa9a-419b2f73cb29",
   "metadata": {},
   "source": [
    "### Loading dataset from bio format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0870f7a-1aa2-4555-bbe6-333fb89312d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      sentence_id     token       label\n",
      "0               0     Mount  B-MOUNTAIN\n",
      "1               0   Everest  I-MOUNTAIN\n",
      "2               0        is           O\n",
      "3               0       the           O\n",
      "4               0   highest           O\n",
      "...           ...       ...         ...\n",
      "1660          104  Northern           O\n",
      "1661          104       and           O\n",
      "1662          104  Southern           O\n",
      "1663          104     India  B-MOUNTAIN\n",
      "1664          104         .           O\n",
      "\n",
      "[1665 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "def load_bio_dataset_to_dataframe(file_path):\n",
    "    data = []\n",
    "    sentence_id = 0\n",
    "\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:  # End of a sentence\n",
    "                sentence_id += 1\n",
    "            else:\n",
    "                token, label = line.split()\n",
    "                data.append({\"sentence_id\": sentence_id, \"token\": token, \"label\": label})\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Load dataset into a DataFrame\n",
    "file_path = \"D:\\\\InterviewProject\\\\Task1\\\\dataset\\\\improved_data.txt\"  # Replace with your file path\n",
    "df = load_bio_dataset_to_dataframe(file_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f46304-46ad-49f8-8a2b-eecd73ed0e42",
   "metadata": {},
   "source": [
    "### Giving labels an integer representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "885a20b8-f9b4-4cc9-83e8-db515d85c232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-MOUNTAIN', 'I-MOUNTAIN', 'O']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df['label'].unique().tolist()\n",
    "labels = [s.strip() for s in labels ]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84316658-d3e6-4e4e-8d63-77ea6e0b4eaf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-MOUNTAIN 0\n",
      "I-MOUNTAIN 1\n",
      "O 2\n"
     ]
    }
   ],
   "source": [
    "for key, value in enumerate(labels):\n",
    "    print(value, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ad0fedb-344e-445d-a977-ce3ae23b6f14",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_LABELS= len(labels)\n",
    "\n",
    "id2label={id:label for id,label in enumerate(labels)}\n",
    "\n",
    "label2id={label:id for id,label in enumerate(labels)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58441c38-178a-4414-a286-67b6e16a3884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-MOUNTAIN': 0, 'I-MOUNTAIN': 1, 'O': 2}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "109c232f-5148-432b-a4e4-1d34d12e4b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'B-MOUNTAIN', 1: 'I-MOUNTAIN', 2: 'O'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9ba1a94-6e68-425b-8fc3-f9d7977e3593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Mount</td>\n",
       "      <td>B-MOUNTAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Everest</td>\n",
       "      <td>I-MOUNTAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>is</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>highest</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id    token       label\n",
       "0            0    Mount  B-MOUNTAIN\n",
       "1            0  Everest  I-MOUNTAIN\n",
       "2            0       is           O\n",
       "3            0      the           O\n",
       "4            0  highest           O"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32540261-dc33-42af-86dc-631be3326399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "      <th>ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Mount</td>\n",
       "      <td>B-MOUNTAIN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Everest</td>\n",
       "      <td>I-MOUNTAIN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>is</td>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>highest</td>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1660</th>\n",
       "      <td>104</td>\n",
       "      <td>Northern</td>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1661</th>\n",
       "      <td>104</td>\n",
       "      <td>and</td>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1662</th>\n",
       "      <td>104</td>\n",
       "      <td>Southern</td>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>104</td>\n",
       "      <td>India</td>\n",
       "      <td>B-MOUNTAIN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1664</th>\n",
       "      <td>104</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1665 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentence_id     token       label  ids\n",
       "0               0     Mount  B-MOUNTAIN    0\n",
       "1               0   Everest  I-MOUNTAIN    1\n",
       "2               0        is           O    2\n",
       "3               0       the           O    2\n",
       "4               0   highest           O    2\n",
       "...           ...       ...         ...  ...\n",
       "1660          104  Northern           O    2\n",
       "1661          104       and           O    2\n",
       "1662          104  Southern           O    2\n",
       "1663          104     India  B-MOUNTAIN    0\n",
       "1664          104         .           O    2\n",
       "\n",
       "[1665 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"ids\"]=df.label.map(lambda x: label2id[x.strip()])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f01b801-3933-4458-90f4-f5452984129d",
   "metadata": {},
   "source": [
    "### Tokenize and allign labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54eb91d0-d8ec-4990-b98c-16fc1a093723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prepare_data(df):\n",
    "    sentences = df.groupby(\"sentence_id\")[\"token\"].apply(list).tolist()\n",
    "    labels = df.groupby(\"sentence_id\")[\"label\"].apply(list).tolist()\n",
    "    return sentences, labels\n",
    "\n",
    "# Prepare tokens and labels\n",
    "sentences, labels = prepare_data(df)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_sentences, test_sentences, train_labels, test_labels = train_test_split(\n",
    "    sentences, labels, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8bf84356-0dd4-4fde-b2d5-6b3d6e79940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load BERT tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize and align labels\n",
    "def tokenize_and_align_labels(sentences, labels):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        sentences,\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    aligned_labels = []\n",
    "    for i, label in enumerate(labels):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to word indices\n",
    "        label_ids = []\n",
    "        previous_word_id = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)  # Ignore special tokens\n",
    "            elif word_id != previous_word_id:\n",
    "                label_ids.append(label2id[label[word_id]])  # Assign label to first subword\n",
    "            else:\n",
    "                label_ids.append(-100)  # Ignore other subword parts\n",
    "            previous_word_id = word_id\n",
    "        aligned_labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = aligned_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Tokenize train and test datasets\n",
    "train_inputs = tokenize_and_align_labels(train_sentences, train_labels)\n",
    "test_inputs = tokenize_and_align_labels(test_sentences, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52650df9-923a-4e88-847e-4450f4cf0471",
   "metadata": {},
   "source": [
    "### Loading pre-trained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42d41af6-5ba9-4503-acf1-4a3eb49dbb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label2id)  # Number of unique labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575d2bdc-4311-4a87-8e13-69bee17c079b",
   "metadata": {},
   "source": [
    "### Setting up trainning of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b45f64a2-eefa-455f-94bf-68f179469902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kiriz\\AppData\\Local\\Temp\\ipykernel_4672\\3428728654.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.316835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.526200</td>\n",
       "      <td>0.206461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.526200</td>\n",
       "      <td>0.171234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18, training_loss=0.40284520387649536, metrics={'train_runtime': 63.5266, 'train_samples_per_second': 3.967, 'train_steps_per_second': 0.283, 'total_flos': 3601028485152.0, 'train_loss': 0.40284520387649536, 'epoch': 3.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import accelerate\n",
    "import transformers\n",
    "\n",
    "\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, inputs):\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val[idx] for key, val in self.inputs.items()}\n",
    "\n",
    "# Create DataLoader-compatible datasets\n",
    "train_dataset = NERDataset(train_inputs)\n",
    "test_dataset = NERDataset(test_inputs)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471d419b-ae55-4b9c-bfa4-2e900353ef3e",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "840ed7e5-4b92-44bf-a8b4-b9e49086dea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_bert_ner\\\\tokenizer_config.json',\n",
       " './fine_tuned_bert_ner\\\\special_tokens_map.json',\n",
       " './fine_tuned_bert_ner\\\\vocab.txt',\n",
       " './fine_tuned_bert_ner\\\\added_tokens.json',\n",
       " './fine_tuned_bert_ner\\\\tokenizer.json')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./fine_tuned_bert_ner\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_bert_ner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b87f74-1330-43d7-9501-d1723b724a4c",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5ad85cac-deb0-47b3-9612-0362a26fc8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  B-MOUNTAIN       0.84      0.92      0.88        39\n",
      "  I-MOUNTAIN       0.91      0.62      0.74        16\n",
      "           O       0.98      0.99      0.98       282\n",
      "\n",
      "    accuracy                           0.96       337\n",
      "   macro avg       0.91      0.84      0.87       337\n",
      "weighted avg       0.96      0.96      0.96       337\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "\n",
    "# Predictions\n",
    "predictions, labels, _ = trainer.predict(test_dataset)\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Convert IDs to tags\n",
    "true_tags = [[id2label[label_id] for label_id in sentence if label_id != -100] for sentence in labels]\n",
    "pred_tags = [[id2label[pred_id] for pred_id, label_id in zip(sentence, labels[i]) if label_id != -100] for i, sentence in enumerate(predictions)]\n",
    "\n",
    "# Flatten the lists of true and predicted tags\n",
    "flat_true_tags = [tag for sentence in true_tags for tag in sentence]\n",
    "flat_pred_tags = [tag for sentence in pred_tags for tag in sentence]\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(flat_true_tags, flat_pred_tags,zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bc5390-c92c-4f58-ab95-673756fba767",
   "metadata": {},
   "source": [
    "### Perform inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c64a6a2a-c67f-4622-bf1c-a7f29d66cff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'LABEL_0', 'score': 0.4456879, 'word': 'mount everest', 'start': 0, 'end': 13}, {'entity_group': 'LABEL_2', 'score': 0.9871081, 'word': 'is one of the tallest peaks in the world.', 'start': 14, 'end': 55}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "ner_pipeline = pipeline(\"ner\", model=model, device=device,tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Test the pipeline\n",
    "text = \"Mount Everest is one of the tallest peaks in the world.\"\n",
    "result = ner_pipeline(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41412264-8570-4d7c-ad18-9a672fd72f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
